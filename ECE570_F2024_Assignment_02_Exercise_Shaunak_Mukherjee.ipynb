{"cells":[{"cell_type":"markdown","metadata":{"id":"kLTpDVQk0ozE"},"source":["# ECE 57000 Assignment 2 Exercises\n","\n","\n","\n","Name: Shaunak Mukherjee"]},{"cell_type":"markdown","metadata":{"id":"APWN2dfm7fx_"},"source":["# Important submission information\n","\n","1. Follow the instructions in the provided \"uploader.ipynb\" to convert your ipynb file into PDF format.\n","2. Please make sure to select the corresponding pages for each exercise when you submitting your PDF to Gradescope. Make sure to include both the **output** and the **code** when selecting pages. (You do not need to include the instruction for the exercises)\n","\n","\n","**We may assess a 20% penalty for those who do not correctly follow these steps.**"]},{"cell_type":"markdown","metadata":{"id":"cZ5oVoEqVnAD"},"source":["# 1. Task description & Background\n","## 1-1. Task description\n","\n","In this assignment, students will implement Stochastic Gradient Descent (SGD) for logistic regression and apply backpropagation for gradient descent/SGD on neural networks. You are only allowed to use basic functions or equivalent operations of NumPy package. The dataset from Assignment 1 will be reused.\n","\n","For the first part (logistic regression), students will define the model, loss function, compute gradients, and implement the SGD algorithm. In the second part, students will implement GD/SGD for a three-layer neural network, focusing on the forward pass and backpropagation.\n","\n","## 1-2. Background on dataset\n","In this assignment, we will explore the application of logistic regression to a binary classification problem in the field of medical diagnostics similar to the first assignment. The objective is to predict whether a breast tumor is benign or malignant based on features extracted from digitized images of fine needle aspirate (FNA) of breast mass.\n","\n","The dataset used is the Breast Cancer dataset from the UCI Machine Learning Repository, incorporated into scikit-learn as `load_breast_cancer`. This dataset includes measurements from 569 instances of breast tumors, with each instance described by 30 numeric attributes. These features include things like the texture, perimeter, smoothness, and symmetry of the tumor cells.\n","\n","You will split the data into training and test sets, with 80% of the data used for training and the remaining 20% for testing. This setup tests the model’s ability to generalize to new, unseen data. We set the `random_state` as 42 to ensure reproducibility. The logistic regression model, initialized with the 'liblinear' solver, will be trained on the training set.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yZCUDuPbVnAj"},"source":["# 2. Loading and preprocessing data from the previous assignment\n","\n","\n","You can load the Breast Cancer dataset by using [this function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) from the `sklearn.datasets` module (we have imported the function for you). Refer to the official documentation to understand more about this function.\n","\n","**Implement the Following:**\n","1.  `data`: Use the built-in function to load the dataset and store it in this variable.\n","2.  `X`: This should store the feature matrix from the dataset.\n","3.  `y`: This should store the target vector, which includes the labels indicating whether the tumor is benign or malignant.\n","\n","`X_train, X_test, y_train, y_test`: Split `X` and `y` into training and testing sets.\n","   - Set `test_size` to 0.2, allocating 20% of the data for testing.\n","   - Use `random_state=42` to ensure that your results are reproducible.\n"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4203,"status":"ok","timestamp":1727237343396,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"I9XT8gbcVnAk","outputId":"7edfc0bf-a689-4541-8dd5-e1ddfa15d6af"},"outputs":[{"name":"stdout","output_type":"stream","text":["The data has a shape of (569, 30), and the target has a shape of (569,)\n","The training set has 455 datapoints and the test set has 114 datapoints.\n","The max of training data is 1.00 and the min is 0.00.\n"]}],"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","print(f'The data has a shape of {X.shape}, and the target has a shape of {y.shape}')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(f'The training set has {X_train.shape[0]} datapoints and the test set has {X_test.shape[0]} datapoints.')\n","\n","scaler = MinMaxScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","print(f'The max of training data is {X_train.max():.2f} and the min is {X_train.min():.2f}.')"]},{"cell_type":"markdown","metadata":{"id":"GlKCbomPXEvo"},"source":["# 3. Initialize and train the logistic regression model with SGD (60/100 points)\n","\n","\n","You will initialize and train a logistic regression model.\n"]},{"cell_type":"markdown","metadata":{"id":"4-wFc_8AXL0L"},"source":["## 3-1. Defining sigmoid function and binary cross entropy function (10/100 points)\n","**Implement the Following:**\n","1. Sigmoid function: Implement the sigmoid function, which takes in a scalar or vector and returns the sigmoid of the input.\n","2. Binary Cross-Entropy Loss: Implement the binary cross-entropy loss function, which takes in the predictions and the true labels and returns the loss value. It is formulated as $\\ell(y,\\hat{y})=-\\frac{1}{N} \\sum_{n=1}^{N} \\left[ y_n( \\log \\hat{y}_n ) + (1-y_n) \\log (1-\\hat{y}_n) \\right]$.\n","\n","Please implement by using basic functions in numpy.\n","Ensure your code is placed between the comments `<Your code>` and `<end code>`. This structure is intended to keep your implementation organized and straightforward.\n"]},{"cell_type":"code","execution_count":208,"metadata":{"id":"2RaZ2U_WVnAl"},"outputs":[],"source":["import numpy as np\n","# initialize numpy random seed\n","np.random.seed(29)\n","\n","# Sigmoid function for logistic regression\n","def sigmoid(z):\n","    # <Your code>\n","\n","    \"\"\"\n","    Computes sigmoid function on Z (scalar or vector)\n","    parameter z: scalar or vector\n","    return: sigmoid of z\n","    \"\"\"\n","\n","    return 1 / (1 + np.exp(-z))\n","\n","    # <end code>\n","\n","# Binary Cross-Entropy Loss\n","def binary_cross_entropy(y_true, y_pred):\n","    # Avoid log(0) by clipping predictions\n","\n","    # <Your code>\n","\n","    \"\"\"\n","    Computes binary cross-entropy loss between true labels and predictions\n","    parameter y_true: true label\n","    parameter y_pred: prediction\n","    return: binary cross-entropy loss\n","    Clipped predicted values to avoid log(0) numerical instability issues in loss calculation\n","    and ensures y_pred is never exactly 0 or 1, preventing undefined log operations\n","    \"\"\"\n","\n","    epsilon = 1e-15\n","    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # This code avoids log(0)\n","\n","    # Computation of binary cross-entropy loss\n","    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n","    return loss\n","\n","    # <end code>"]},{"cell_type":"markdown","metadata":{"id":"S_nH8N9iWoRs"},"source":["## 3-2. Defining a class `LogisticRegression` based on SGD (50/100 points)\n","\n","**Implement the Following:**\n","1. Initialize Parameters: Implement `initialize_weights` function. Use zero initialization for `weights` and `bias`.\n","\n","2. predict function: Implement the predict function, which takes in the feature matrix `X` and returns the predicted value. Assuming $W \\in \\mathbb{R}^{D}$, $X \\in \\mathbb{R}^{N \\times D}$, $b \\in \\mathbb{R}^{1}$, the linear model is defined as $\\sigma(XW + b)$, where $\\sigma$ is sigmoid function.\n","\n","3. fit function: Implement the fit function, which trains the logistic regression model using SGD. The function should take in the feature matrix `X`, the true labels `y`, the learning rate `lr`, and the number of epochs `n_epochs`. In specific, first, at every epoch, you may shuffle indices of `n_samples` and reorganize the order of `X` and `y` to make sure that the order is randomized per epoch. Second, make a for loop for SGD. You may want to make a small batch data like `X_batch` and `y_batch`. Third, inside of the for loop for SGD, make a prediction by using the `predict` function you implemented. Fourth, compute the gradient with respect to `weights` and with respect to `bias`. Fifth, use the gradient to update `weights` and `bias`. In other words, implement the SGD algorithm $w^{(1)}=w^{(0)}-\\alpha \\nabla_w (\\text{BCE} (y, \\hat{y} ) )$ and $b^{(1)}=b^{(0)}-\\alpha \\nabla_b ( \\text{BCE} ( y,\\hat{y} ))$, where $\\alpha$ is a learning rate and $\\hat{y}$ is the prediction $\\sigma(XW + b)$. BCE indicates binary cross entropy loss.\n","\n","You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.   \n","Make sure you get accuracy greater than **0.85** on the test set."]},{"cell_type":"code","execution_count":209,"metadata":{"id":"kfTGYGVIWinN"},"outputs":[],"source":["class LogisticRegression_SGD:\n","    def __init__(self, learning_rate=0.01, epochs=100, batch_size=32):\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","\n","    # Initialize weights\n","    def initialize_weights(self, n_features):\n","        \"\"\"\n","        Initializes weights and bias to zero.\n","        :param n_features: Number of input features\n","        \"\"\"\n","        # <Your code>\n","\n","        # Zero initialization for weights and bias\n","        self.weights = np.zeros(n_features)\n","        self.bias = 0\n","\n","        # <end code>\n","\n","\n","    # Prediction function\n","    def predict(self, X):\n","        # <Your code>\n","\n","        \"\"\"\n","        Predicts class labels for the input data.\n","        parameter X: Input data\n","        return: Predicted class labels\n","        \"\"\"\n","        # Prediction function using the sigmoid on the linear\n","        # model XW + b provided above\n","\n","        linear_model = np.dot(X, self.weights) + self.bias\n","        predictions = sigmoid(linear_model)\n","        return predictions\n","\n","        # <end code>\n","\n","    # Training function using mini-batch SGD, \n","    # verbose trigger here simply for printing loss information\n","    def fit(self, X, y, verbose=True): \n","        n_samples, n_features = X.shape\n","        self.initialize_weights(n_features)\n","\n","        for epoch in range(self.epochs):\n","            # Shuffle the data\n","            indices = np.arange(n_samples)\n","            np.random.shuffle(indices)\n","            X = X[indices]\n","            y = y[indices]\n","\n","            if epoch == 0:\n","              loss = binary_cross_entropy(y, self.predict(X))\n","              if verbose:\n","                print(\"SGD loss\")\n","                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n","\n","            for i in range(0, n_samples, self.batch_size):\n","                X_batch = X[i:i + self.batch_size]\n","                y_batch = y[i:i + self.batch_size]\n","\n","                # <Your code>\n","\n","                # Predictions\n","                y_pred = self.predict(X_batch)\n","\n","                # Compute gradients\n","                dw = np.dot(X_batch.T, (y_pred - y_batch)) / self.batch_size\n","                db = np.sum(y_pred - y_batch) / self.batch_size\n","\n","                # Update weights\n","                self.weights = self.weights - self.learning_rate * dw\n","\n","                # Update bias\n","                self.bias = self.bias - self.learning_rate * db\n","\n","\n","                # <end code>\n","\n","            # Calculate loss for monitoring\n","            loss = binary_cross_entropy(y, self.predict(X))\n","            if verbose:\n","              if (epoch + 1) % 10 == 0:\n","                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")"]},{"cell_type":"code","execution_count":210,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":465,"status":"ok","timestamp":1727237343858,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"iqqADhS7Vj2W","outputId":"2d5ee736-7857-466e-8436-fb123c4a88ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["SGD loss\n","Epoch 1/100, Loss: 0.6931\n","Epoch 10/100, Loss: 0.1523\n","Epoch 20/100, Loss: 0.1212\n","Epoch 30/100, Loss: 0.1055\n","Epoch 40/100, Loss: 0.0970\n","Epoch 50/100, Loss: 0.0915\n","Epoch 60/100, Loss: 0.0868\n","Epoch 70/100, Loss: 0.0841\n","Epoch 80/100, Loss: 0.0800\n","Epoch 90/100, Loss: 0.0779\n","Epoch 100/100, Loss: 0.0757\n"]}],"source":["# You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","# Training the model\n","model_SGD = LogisticRegression_SGD(learning_rate=1, epochs=100, batch_size=16)\n","model_SGD.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":211,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1727237344143,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"CTnF9h3yLUOW","outputId":"72f5932e-b902-4b0b-b278-9aa881671d5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 0 0]\n","[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","The accuracy is 0.9737\n"]}],"source":["# Code to check accuracy of your implementation\n","from sklearn.metrics import accuracy_score\n","\n","predictions = model_SGD.predict(X_test)\n","predictions = (predictions > 0.5).astype(int)\n","print(predictions)\n","print(y_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f'The accuracy is {accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"0ij7MUZem8kj"},"source":["### Experimenting with hyperparameters tuning used earlier to improve accuracy."]},{"cell_type":"code","execution_count":212,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23562,"status":"ok","timestamp":1727273852806,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"bn0kAG2jpHKn","outputId":"9ac029ef-15f7-4846-cfff-e6e79a2c9b8a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:   0%|          | 0/72 [00:00<?, ?iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 100, Learning Rate: 0.001, Batch Size: 16, the corresponding Accuracy is: 0.8246\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:   4%|▍         | 3/72 [00:00<00:02, 23.89iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 100, Learning Rate: 0.001, Batch Size: 32, the corresponding Accuracy is: 0.7544\n","For Epochs: 100, Learning Rate: 0.001, Batch Size: 64, the corresponding Accuracy is: 0.7105\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:   8%|▊         | 6/72 [00:00<00:02, 26.48iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 100, Learning Rate: 0.01, Batch Size: 16, the corresponding Accuracy is: 0.9474\n","For Epochs: 100, Learning Rate: 0.01, Batch Size: 32, the corresponding Accuracy is: 0.9386\n","For Epochs: 100, Learning Rate: 0.01, Batch Size: 64, the corresponding Accuracy is: 0.8947\n","For Epochs: 100, Learning Rate: 0.05, Batch Size: 16, the corresponding Accuracy is: 0.9561"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  12%|█▎        | 9/72 [00:00<00:02, 22.91iteration/s]"]},{"name":"stdout","output_type":"stream","text":["\n","For Epochs: 100, Learning Rate: 0.05, Batch Size: 32, the corresponding Accuracy is: 0.9386\n","For Epochs: 100, Learning Rate: 0.05, Batch Size: 64, the corresponding Accuracy is: 0.9474\n","For Epochs: 100, Learning Rate: 0.1, Batch Size: 16, the corresponding Accuracy is: 0.9649\n","For Epochs: 100, Learning Rate: 0.1, Batch Size: 32, the corresponding Accuracy is: 0.9561\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  21%|██        | 15/72 [00:00<00:02, 24.37iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 100, Learning Rate: 0.1, Batch Size: 64, the corresponding Accuracy is: 0.9386\n","For Epochs: 100, Learning Rate: 0.5, Batch Size: 16, the corresponding Accuracy is: 0.9737\n","For Epochs: 100, Learning Rate: 0.5, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 100, Learning Rate: 0.5, Batch Size: 64, the corresponding Accuracy is: 0.9649\n","For Epochs: 100, Learning Rate: 1, Batch Size: 16, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  25%|██▌       | 18/72 [00:00<00:02, 23.01iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 100, Learning Rate: 1, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 100, Learning Rate: 1, Batch Size: 64, the corresponding Accuracy is: 0.9737\n","For Epochs: 200, Learning Rate: 0.001, Batch Size: 16, the corresponding Accuracy is: 0.8772\n","For Epochs: 200, Learning Rate: 0.001, Batch Size: 32, the corresponding Accuracy is: 0.8246\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  29%|██▉       | 21/72 [00:00<00:02, 18.97iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 200, Learning Rate: 0.001, Batch Size: 64, the corresponding Accuracy is: 0.7544\n","For Epochs: 200, Learning Rate: 0.01, Batch Size: 16, the corresponding Accuracy is: 0.9474\n","For Epochs: 200, Learning Rate: 0.01, Batch Size: 32, the corresponding Accuracy is: 0.9474\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  33%|███▎      | 24/72 [00:01<00:02, 16.10iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 200, Learning Rate: 0.01, Batch Size: 64, the corresponding Accuracy is: 0.9386\n","For Epochs: 200, Learning Rate: 0.05, Batch Size: 16, the corresponding Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  39%|███▉      | 28/72 [00:01<00:03, 13.84iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 200, Learning Rate: 0.05, Batch Size: 32, the corresponding Accuracy is: 0.9561\n","For Epochs: 200, Learning Rate: 0.05, Batch Size: 64, the corresponding Accuracy is: 0.9386\n","For Epochs: 200, Learning Rate: 0.1, Batch Size: 16, the corresponding Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  42%|████▏     | 30/72 [00:01<00:02, 14.31iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 200, Learning Rate: 0.1, Batch Size: 32, the corresponding Accuracy is: 0.9649\n","For Epochs: 200, Learning Rate: 0.1, Batch Size: 64, the corresponding Accuracy is: 0.9561\n","For Epochs: 200, Learning Rate: 0.5, Batch Size: 16, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  47%|████▋     | 34/72 [00:02<00:02, 13.01iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 200, Learning Rate: 0.5, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 200, Learning Rate: 0.5, Batch Size: 64, the corresponding Accuracy is: 0.9649\n","For Epochs: 200, Learning Rate: 1, Batch Size: 16, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  50%|█████     | 36/72 [00:02<00:02, 13.31iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 200, Learning Rate: 1, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 200, Learning Rate: 1, Batch Size: 64, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  53%|█████▎    | 38/72 [00:02<00:03, 10.01iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.001, Batch Size: 16, the corresponding Accuracy is: 0.9123\n","For Epochs: 300, Learning Rate: 0.001, Batch Size: 32, the corresponding Accuracy is: 0.8684\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  56%|█████▌    | 40/72 [00:02<00:03,  9.24iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.001, Batch Size: 64, the corresponding Accuracy is: 0.7807\n","For Epochs: 300, Learning Rate: 0.01, Batch Size: 16, the corresponding Accuracy is: 0.9386\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  58%|█████▊    | 42/72 [00:02<00:03,  9.99iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.01, Batch Size: 32, the corresponding Accuracy is: 0.9474\n","For Epochs: 300, Learning Rate: 0.01, Batch Size: 64, the corresponding Accuracy is: 0.9474\n","For Epochs: 300, Learning Rate: 0.05, Batch Size: 16, the corresponding Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  61%|██████    | 44/72 [00:03<00:02,  9.41iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.05, Batch Size: 32, the corresponding Accuracy is: 0.9649\n","For Epochs: 300, Learning Rate: 0.05, Batch Size: 64, the corresponding Accuracy is: 0.9561\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  65%|██████▌   | 47/72 [00:03<00:02,  8.87iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.1, Batch Size: 16, the corresponding Accuracy is: 0.9737\n","For Epochs: 300, Learning Rate: 0.1, Batch Size: 32, the corresponding Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  68%|██████▊   | 49/72 [00:03<00:02,  8.61iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.1, Batch Size: 64, the corresponding Accuracy is: 0.9649\n","For Epochs: 300, Learning Rate: 0.5, Batch Size: 16, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  69%|██████▉   | 50/72 [00:03<00:02,  8.27iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 0.5, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 300, Learning Rate: 0.5, Batch Size: 64, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  74%|███████▎  | 53/72 [00:04<00:02,  8.38iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 300, Learning Rate: 1, Batch Size: 16, the corresponding Accuracy is: 0.9737\n","For Epochs: 300, Learning Rate: 1, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 300, Learning Rate: 1, Batch Size: 64, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  78%|███████▊  | 56/72 [00:04<00:02,  7.33iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.001, Batch Size: 16, the corresponding Accuracy is: 0.9386\n","For Epochs: 400, Learning Rate: 0.001, Batch Size: 32, the corresponding Accuracy is: 0.8772\n","For Epochs: 400, Learning Rate: 0.001, Batch Size: 64, the corresponding Accuracy is: 0.8246\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  82%|████████▏ | 59/72 [00:05<00:01,  7.17iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.01, Batch Size: 16, the corresponding Accuracy is: 0.9561\n","For Epochs: 400, Learning Rate: 0.01, Batch Size: 32, the corresponding Accuracy is: 0.9474\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  85%|████████▍ | 61/72 [00:05<00:01,  7.61iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.01, Batch Size: 64, the corresponding Accuracy is: 0.9474\n","For Epochs: 400, Learning Rate: 0.05, Batch Size: 16, the corresponding Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  88%|████████▊ | 63/72 [00:05<00:01,  7.67iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.05, Batch Size: 32, the corresponding Accuracy is: 0.9649\n","For Epochs: 400, Learning Rate: 0.05, Batch Size: 64, the corresponding Accuracy is: 0.9561\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  90%|█████████ | 65/72 [00:06<00:01,  6.91iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.1, Batch Size: 16, the corresponding Accuracy is: 0.9737\n","For Epochs: 400, Learning Rate: 0.1, Batch Size: 32, the corresponding Accuracy is: 0.9649\n","For Epochs: 400, Learning Rate: 0.1, Batch Size: 64, the corresponding Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  94%|█████████▍| 68/72 [00:06<00:00,  6.69iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.5, Batch Size: 16, the corresponding Accuracy is: 0.9737\n","For Epochs: 400, Learning Rate: 0.5, Batch Size: 32, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  96%|█████████▌| 69/72 [00:06<00:00,  6.75iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 0.5, Batch Size: 64, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation:  97%|█████████▋| 70/72 [00:06<00:00,  5.66iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 1, Batch Size: 16, the corresponding Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning Experimentation: 100%|██████████| 72/72 [00:07<00:00,  9.98iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 400, Learning Rate: 1, Batch Size: 32, the corresponding Accuracy is: 0.9737\n","For Epochs: 400, Learning Rate: 1, Batch Size: 64, the corresponding Accuracy is: 0.9737\n","\n","The Best Accuracy hyper-parameter Settings:\n","For Epochs: 100, Learning Rate: 0.5 and Batch Size: 16, we can get best accuracy: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","\n","\"\"\"\n","Below is an experiment with varying hyperparameters such as epochs, learning rates,\n","and batch sizes. The idea here is to find the best combination of hyperparameters\n","that yields the highest accuracy.\n","\"\"\"\n","\n","# List of hyperparameters to vary\n","epochs_list = [100, 200, 300, 400]  # Varying epochs\n","learning_rates = [0.001, 0.01, 0.05, 0.1, 0.5, 1]  # Varying learning rates\n","batches = [16, 32, 64]  # Varying batch sizes\n","accuracy_results = []  # Empty list to store computed accuracies\n","\n","# Initialize variables\n","best_accuracy = 0\n","best_epochs = 0\n","best_lr = 0\n","best_batch = 0\n","\n","# Total iterations for tqdm\n","total_iterations = len(epochs_list) * len(learning_rates) * len(batches)\n","\n","\"\"\"\n","Implement tqdm progress bar for the entire process (Reference: https://github.com/tqdm/tqdm)\n","\"\"\"\n","\n","with tqdm(total=total_iterations, desc='Hyperparameter Tuning Experimentation', unit='iteration') as progress_bar:\n","    # Nested loops to iterate over epochs, learning rates, and batch sizes\n","    for epochs in epochs_list:\n","        for lr in learning_rates:\n","            for batch in batches:\n","                # Initialize and fit the model for each iteration\n","                model_SGD = LogisticRegression_SGD(learning_rate=lr, epochs=epochs, batch_size=batch)\n","                model_SGD.fit(X_train, y_train, verbose=False)\n","\n","                # Make predictions and calculate accuracy\n","                predictions = model_SGD.predict(X_test)\n","                predictions = (predictions > 0.5).astype(int)\n","                accuracy = accuracy_score(y_test, predictions)\n","\n","                # Store\n","                accuracy_results.append((epochs, lr, batch, accuracy))\n","\n","                # Print results\n","                print(f'For Epochs: {epochs}, Learning Rate: {lr}, Batch Size: {batch}, the corresponding Accuracy is: {accuracy:.4f}')\n","\n","                # Check if this accuracy is better than the best found so far\n","                if accuracy > best_accuracy:\n","                    best_accuracy = accuracy\n","                    best_epochs = epochs\n","                    best_lr = lr\n","                    best_batch = batch\n","\n","                # Update progress bar\n","                progress_bar.update(1)\n","\n","# Print the best hyper-parameter settings and corresponding accuracy\n","print(\"\\nThe Best Accuracy hyper-parameter Settings:\")\n","print(f'For Epochs: {best_epochs}, Learning Rate: {best_lr} and Batch Size: {best_batch}, we can get best accuracy: {best_accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"NXsERLmJVnAm"},"source":["# 4. 3-Layer Neural Network with SGD (40/100 points)\n","\n","\n","Now, we extend our 1-layer neural network to 3-layers neural network.\n","\n","**Implement the Following:**\n","\n","Ensure your code is placed between the comments `<Your code>` and `<end code>`. This structure is intended to keep your implementation organized and straightforward.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6Kk26XbDXL0N"},"source":["## 4-1. Defining activation functions and the derivative (10/100 points)\n","\n","**Implement the Following:**\n","1. relu function: Implement the ReLU activation function, which takes in a scalar or vector and returns the ReLU of the input.\n","2. relu_derivative function: Implement the derivative of the ReLU activation function, which takes in a scalar or vector and returns the derivative of the ReLU of the input.\n","3. sigmoid function: Implement the sigmoid activation function, which takes in a scalar or vector and returns the sigmoid of the input."]},{"cell_type":"code","execution_count":213,"metadata":{"id":"gB2LyOiHYhWY"},"outputs":[],"source":["def relu(z):\n","    \"\"\"ReLU activation function.\"\"\"\n","    # <Your code>\n","\n","    return np.maximum(0.0, z)\n","\n","    # <end code>\n","\n","def relu_derivative(z):\n","    \"\"\"Derivative of ReLU activation function.\"\"\"\n","    # <Your code>\n","    return (z > 0).astype(int)\n","\n","    # <end code>\n","\n","def sigmoid(z):\n","    \"\"\"Sigmoid activation function.\"\"\"\n","    # <Your code>\n","    return 1 / (1 + np.exp(-z))\n","\n","    # <end code>\n"]},{"cell_type":"markdown","metadata":{"id":"ExSubVgGYrA5"},"source":["## 4-2. Defining 3-layer Neural Network (30/100 points)\n","\n","**Implement the Following:**\n","\n","1. Initialize Parameters: Implement `initialize_weights` function. Implement Kaiming initialization to initialize `weights`. Use zero initialization for `bias`.\n","\n","2. forward function: Compute the pre-activation for each layer by multiplying inputs or previous activations with weights and adding biases. Apply the ReLU activation function for hidden layers and the Sigmoid function for the output layer. Finally, return the activated output of the network. The formulation of forward function can be defined as:\n","\n","$$\\sigma(\\text{relu}(\\text{relu}(XW_1+b_1)W_2+b_2)W_3+b_3)$$\n","\n","3. backward function:\n","    1. **Compute Gradient of Loss**: Calculate the gradient of the loss with respect to the network's output.\n","    2. **Compute Gradients for Weights and Biases**: Use the gradients from the output to compute the gradients of weights and biases at each layer, applying the activation function's derivative where needed.\n","    3. **Propagate Gradients Backward**: Continue to backpropagate the gradients through the network, adjusting calculations as you move from one layer to the previous.\n","    4. **Update Parameters**: Update all weights and biases using the calculated gradients and learning rate.\n","\n","    This backpropagation adjusts the model parameters to minimize the loss.\n","\n","4. predict function: Implement the predict function, which takes in the feature matrix `X` and returns the predicted class (0 or 1).\n","\n","You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","\n","Make sure you get accuracy greater than **0.75** on the test set."]},{"cell_type":"code","execution_count":214,"metadata":{"id":"Uy8RcFLdVnAn"},"outputs":[],"source":["# Neural Network Model with an additional hidden layer\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, learning_rate=0.01, epochs=100, batch_size=32):\n","        \"\"\"\n","        Initialize the Neural Network with given parameters.\n","        :param input_size: Number of input features\n","        :param hidden_size1: Number of neurons in the first hidden layer\n","        :param hidden_size2: Number of neurons in the second hidden layer\n","        :param output_size: Number of output neurons (1 for binary classification)\n","        :param learning_rate: Learning rate for weight updates\n","        :param epochs: Number of training iterations\n","        :param batch_size: Size of mini-batches for SGD\n","        \"\"\"\n","        self.input_size = input_size\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.output_size = output_size\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.initialize_weights()\n","        self.loss_history = []\n","\n","    def initialize_weights(self):\n","        \"\"\"Initialize weights and biases using Kaiming initialization.\"\"\"\n","\n","        # <Your code>\n","\n","        # # Kaiming initialization for the first layer weights\n","        self.W1 = np.random.randn(self.input_size, self.hidden_size1) * np.sqrt(2 / self.input_size)\n","        self.b1 = np.zeros(self.hidden_size1)\n","\n","\n","        # Kaiming initialization for the second layer weights\n","        self.W2 = np.random.randn(self.hidden_size1, self.hidden_size2) * np.sqrt(2 / self.hidden_size1)\n","        self.b2 = np.zeros(self.hidden_size2)\n","\n","\n","        # Kaiming initialization for the third layer weights\n","        self.W3 = np.random.randn(self.hidden_size2, self.output_size) * np.sqrt(2 / self.hidden_size2)\n","        self.b3 = np.zeros(self.output_size)\n","\n","        # <end code>\n","\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Forward pass through the network.\n","        :param X: Input data\n","        :return: Activated output of the network\n","        \"\"\"\n","        # <Your code>\n","\n","        # Forward pass for the hidden layer1\n","        self.Z1 = np.dot(X, self.W1) + self.b1\n","        self.A1 = relu(self.Z1)\n","\n","        # Forward pass for the hidden layer2\n","        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n","        self.A2 = relu(self.Z2)\n","\n","        # Forward pass for the output layer\n","        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n","        A3 = sigmoid(self.Z3)\n","\n","        return A3\n","        # <end code>\n","\n","    def backward(self, X, y, output):\n","        \"\"\"\n","        Backpropagation to compute gradients and update weights.\n","        :param X: Input data\n","        :param y: True labels\n","        :param output: Predicted output from forward pass\n","        \"\"\"\n","        m = X.shape[0]\n","\n","        # Gradient of loss w.r.t. output (binary cross-entropy with sigmoid activation)\n","        dZ3 = output - y[:,None]  # Gradient wrt Z3 when using sigmoid activation at output\n","\n","\n","        # <Your code>\n","\n","        # Gradients for the third layer (output layer)\n","        dW3 = (1/m) * np.dot(self.A2.T, dZ3)\n","        db3 = (1/m) * np.sum(dZ3, axis=0)\n","\n","        # Gradients for the second hidden layer\n","        dZ2 = np.dot(dZ3, self.W3.T) * relu_derivative(self.Z2)\n","        dW2 = (1 / m) * np.dot(self.A1.T, dZ2)\n","        db2 = (1 / m) * np.sum(dZ2, axis=0)\n","\n","        # Gradients for the first hidden layer\n","        dZ1 = np.dot(dZ2, self.W2.T) * relu_derivative(self.Z1)\n","        dW1 = (1 / m) * np.dot(X.T, dZ1)\n","        db1 = (1 / m) * np.sum(dZ1, axis=0)\n","\n","        # Update weights and biases \n","        self.W3 = self.W3 - self.learning_rate * dW3\n","        self.b3 = self.b3 - self.learning_rate * db3\n","        \n","        self.W2 = self.W2 - self.learning_rate * dW2\n","        self.b2 = self.b2 - self.learning_rate * db2\n","\n","        self.W1 = self.W1 - self.learning_rate * dW1\n","        self.b1 = self.b1 - self.learning_rate * db1\n","        \n","        # <end code>\n","\n","    def fit(self, X, y, verbose =True):\n","        \"\"\"\n","        Train the neural network using mini-batch SGD.\n","        :param X: Training data\n","        :param y: True labels\n","        \"\"\"\n","        loss = binary_cross_entropy(y, self.forward(X))\n","        if verbose:\n","          print(f\"Epoch 0/{self.epochs}, Loss: {loss:.4f}\")\n","\n","        for epoch in range(self.epochs):\n","            indices = np.arange(X.shape[0])\n","            np.random.shuffle(indices)\n","            X = X[indices]\n","            y = y[indices]\n","\n","            for i in range(0, X.shape[0], self.batch_size):\n","                X_batch = X[i:i + self.batch_size]\n","                y_batch = y[i:i + self.batch_size]\n","\n","                # Forward and backward pass\n","                output = self.forward(X_batch)\n","                self.backward(X_batch, y_batch, output)\n","\n","            # Calculate and print loss for monitoring\n","            loss = binary_cross_entropy(y, self.forward(X))\n","            self.loss_history.append(loss)  # Store loss\n","            if verbose and (epoch + 1) % 100 == 0:\n","                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n","\n","    def get_loss_history(self):\n","            \"\"\"Return the loss history.\"\"\"\n","            return self.loss_history\n","\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict using the trained neural network.\n","        :param X: Input data\n","        :return: Predicted labels\n","        \"\"\"\n","\n","        # <Your code>\n","        predictions = self.forward(X)\n","        return (predictions > 0.5).astype(int)  # Threashold is 0.5 \n","        # <end code>"]},{"cell_type":"code","execution_count":215,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30044,"status":"ok","timestamp":1727237374182,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"JnakwS_5LUOW","outputId":"e25d908a-07ef-4f27-944b-66d25f716ba5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/1000, Loss: 0.7023\n","Epoch 100/1000, Loss: 0.6915\n","Epoch 200/1000, Loss: 0.6821\n","Epoch 300/1000, Loss: 0.6770\n","Epoch 400/1000, Loss: 0.6745\n","Epoch 500/1000, Loss: 0.6734\n","Epoch 600/1000, Loss: 0.6734\n","Epoch 700/1000, Loss: 0.6739\n","Epoch 800/1000, Loss: 0.6749\n","Epoch 900/1000, Loss: 0.6761\n","Epoch 1000/1000, Loss: 0.6776\n"]}],"source":["# # You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","nn_network = NeuralNetwork(input_size=X_train.shape[1], hidden_size1=8, hidden_size2=4, output_size=1, learning_rate=0.0001, epochs=1000, batch_size=32)\n","nn_network.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":216,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1727237374182,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"vjsW0iYKLUOW","outputId":"c0ec48f2-5583-4029-9ae5-cf0b4c399a53"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n"," 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n"," 1 1 0]\n","[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","The accuracy is 0.7807\n"]}],"source":["# Code to check accuracy of your implementation\n","predictions = nn_network.predict(X_test)\n","print(predictions.reshape(-1))\n","print(y_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f'The accuracy is {accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"tgpOEF9FWENb"},"source":["hence greater than 0.75 achieved."]},{"cell_type":"markdown","metadata":{"id":"d_Z-OixglkZh"},"source":["## Experimenting with hyperparameters tuning used earlier to improve accuracy"]},{"cell_type":"code","execution_count":217,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1306582,"status":"ok","timestamp":1727273829257,"user":{"displayName":"Shaunak Mukherjee","userId":"04210674861217377189"},"user_tz":420},"id":"WCfoVKV7S3Vu","outputId":"4359a3e7-82c7-4065-eef9-bc3670358662"},"outputs":[{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:   4%|▎         | 1/27 [00:00<00:16,  1.58iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.0001, Batch Size: 16, the corresponding Loss is 0.6763 and Accuracy is: 0.6228\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:   7%|▋         | 2/27 [00:01<00:13,  1.87iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.0001, Batch Size: 32, the corresponding Loss is 0.6867 and Accuracy is: 0.6754\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  11%|█         | 3/27 [00:01<00:11,  2.16iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.0001, Batch Size: 64, the corresponding Loss is 0.7611 and Accuracy is: 0.3772\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  15%|█▍        | 4/27 [00:02<00:12,  1.88iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.001, Batch Size: 16, the corresponding Loss is 0.9939 and Accuracy is: 0.9649\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  19%|█▊        | 5/27 [00:02<00:11,  1.98iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.001, Batch Size: 32, the corresponding Loss is 1.0118 and Accuracy is: 0.9561\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  22%|██▏       | 6/27 [00:02<00:09,  2.20iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.001, Batch Size: 64, the corresponding Loss is 0.6638 and Accuracy is: 0.6228\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  26%|██▌       | 7/27 [00:03<00:10,  1.87iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.01, Batch Size: 16, the corresponding Loss is 2.5641 and Accuracy is: 0.9825\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  30%|██▉       | 8/27 [00:04<00:09,  1.92iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.01, Batch Size: 32, the corresponding Loss is 3.1302 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  33%|███▎      | 9/27 [00:04<00:08,  2.08iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 500, Learning Rate: 0.01, Batch Size: 64, the corresponding Loss is 2.3665 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  37%|███▋      | 10/27 [00:05<00:12,  1.36iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.0001, Batch Size: 16, the corresponding Loss is 0.6665 and Accuracy is: 0.6228\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  41%|████      | 11/27 [00:06<00:12,  1.26iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.0001, Batch Size: 32, the corresponding Loss is 0.6798 and Accuracy is: 0.6140\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  44%|████▍     | 12/27 [00:07<00:11,  1.27iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.0001, Batch Size: 64, the corresponding Loss is 0.6776 and Accuracy is: 0.6228\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  48%|████▊     | 13/27 [00:08<00:13,  1.06iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.001, Batch Size: 16, the corresponding Loss is 1.6647 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  52%|█████▏    | 14/27 [00:09<00:12,  1.06iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.001, Batch Size: 32, the corresponding Loss is 1.1679 and Accuracy is: 0.9825\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  56%|█████▌    | 15/27 [00:10<00:10,  1.14iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.001, Batch Size: 64, the corresponding Loss is 0.7989 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  59%|█████▉    | 16/27 [00:11<00:11,  1.01s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.01, Batch Size: 16, the corresponding Loss is 3.0212 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  63%|██████▎   | 17/27 [00:12<00:09,  1.02iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.01, Batch Size: 32, the corresponding Loss is 2.4812 and Accuracy is: 0.9825\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  67%|██████▋   | 18/27 [00:13<00:08,  1.10iteration/s]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 1000, Learning Rate: 0.01, Batch Size: 64, the corresponding Loss is 3.0554 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  70%|███████   | 19/27 [00:16<00:11,  1.42s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.0001, Batch Size: 16, the corresponding Loss is 0.8713 and Accuracy is: 0.9474\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  74%|███████▍  | 20/27 [00:17<00:10,  1.55s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.0001, Batch Size: 32, the corresponding Loss is 0.6789 and Accuracy is: 0.7719\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  78%|███████▊  | 21/27 [00:19<00:09,  1.55s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.0001, Batch Size: 64, the corresponding Loss is 0.6676 and Accuracy is: 0.6228\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  81%|████████▏ | 22/27 [00:22<00:09,  1.89s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.001, Batch Size: 16, the corresponding Loss is 2.6559 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  85%|████████▌ | 23/27 [00:24<00:07,  1.89s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.001, Batch Size: 32, the corresponding Loss is 1.6373 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  89%|████████▉ | 24/27 [00:25<00:05,  1.79s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.001, Batch Size: 64, the corresponding Loss is 1.0048 and Accuracy is: 0.9474\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  93%|█████████▎| 25/27 [00:28<00:04,  2.04s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.01, Batch Size: 16, the corresponding Loss is 5.1063 and Accuracy is: 0.9737\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning:  96%|█████████▋| 26/27 [00:30<00:01,  1.98s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.01, Batch Size: 32, the corresponding Loss is 2.9581 and Accuracy is: 0.9825\n"]},{"name":"stderr","output_type":"stream","text":["Hyperparameter Tuning: 100%|██████████| 27/27 [00:31<00:00,  1.17s/iteration]"]},{"name":"stdout","output_type":"stream","text":["For Epochs: 2000, Learning Rate: 0.01, Batch Size: 64, the corresponding Loss is 2.6625 and Accuracy is: 0.9825\n","\n","Best Accuracy Settings:\n","For Epochs: 500, Learning Rate: 0.01, and Batch Size: 16, the Best Accuracy is: 0.9825\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","import numpy as np\n","\n","# List of hyperparameters to vary\n","epochs_list = [500, 1000, 2000]  # Varying epochs\n","learning_rates = [0.0001, 0.001, 0.01]  # Varying learning rates\n","batches = [16, 32, 64]  # Varying batch sizes\n","accuracy_results = []  # Empty list to store computed accuracies\n","\n","# Initialize variables\n","best_accuracy = 0\n","best_epochs = 0\n","best_lr = 0\n","best_batch = 0\n","\n","# Total iterations for tqdm\n","total_iterations = len(epochs_list) * len(learning_rates) * len(batches)\n","\n","# Create a single tqdm progress bar for the entire process\n","with tqdm(total=total_iterations, desc='Hyperparameter Tuning', unit='iteration') as progress_bar:\n","    # Nested loops to iterate over epochs, learning rates, and batch sizes\n","    for epochs in epochs_list:\n","        for lr in learning_rates:\n","            for batch in batches:\n","                # Fit the model for each iteration\n","                nn_model = NeuralNetwork(input_size=X_train.shape[1],\n","                                         hidden_size1=8,\n","                                         hidden_size2=4,\n","                                         output_size=1,\n","                                         learning_rate=lr,\n","                                         epochs=epochs,\n","                                         batch_size=batch)\n","\n","                nn_model.fit(X=X_train, y=y_train, verbose=False)\n","\n","\n","                # Predictions and accuracy calculation\n","                predictions = nn_model.predict(X_test)\n","                predictions = predictions.reshape(-1)\n","                accuracy = accuracy_score(y_test, predictions)\n","                loss_history = nn_model.get_loss_history()\n","\n","                # Store the accuracy result\n","                accuracy_results.append((epochs, lr, batch, accuracy))\n","\n","                # Print results\n","                print(f'For Epochs: {epochs}, Learning Rate: {lr}, Batch Size: {batch}, the corresponding Loss is {loss_history[-1]:.4f} and Accuracy is: {accuracy:.4f}')\n","\n","                # Check if this accuracy is better than previous\n","                if accuracy > best_accuracy:\n","                    best_accuracy = accuracy\n","                    best_epochs = epochs\n","                    best_lr = lr\n","                    best_batch = batch\n","\n","                # Update tqdm progress bar\n","                progress_bar.update(1)\n","\n","# Print the best hyper-parameters and accuracy\n","print(\"\\nBest Accuracy Settings:\")\n","print(f'For Epochs: {best_epochs}, Learning Rate: {best_lr}, and Batch Size: {best_batch}, the Best Accuracy is: {best_accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"02VkfkxOeFae"},"source":["End of Assignment 2"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
